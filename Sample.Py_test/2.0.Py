# Pipeline Machine Learning - PhÃ¢n tÃ­ch tÃ¡c Ä‘á»™ng ICT & IC Ä‘áº¿n tÄƒng trÆ°á»Ÿng kinh táº¿
# Author: AI Assistant
# Date: 2025

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.impute import KNNImputer
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.cluster import KMeans
from sklearn.model_selection import train_test_split
from sklearn.linear_model import Lasso, LassoCV
from sklearn.ensemble import RandomForestRegressor
from sklearn.neural_network import MLPRegressor
from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error
import xgboost as xgb
import warnings

warnings.filterwarnings("ignore")


# Thiáº¿t láº­p style cho plots
plt.style.use("seaborn-v0_8")
sns.set_palette("husl")


class ICT_IC_MLPipeline:
    def __init__(self, data_path=None, df=None):
        """
        Khá»Ÿi táº¡o pipeline ML cho phÃ¢n tÃ­ch ICT & IC

        Parameters:
        - data_path: Ä‘Æ°á»ng dáº«n file CSV
        - df: DataFrame náº¿u Ä‘Ã£ load sáºµn
        """
        if df is not None:
            self.df = df.copy()
        elif data_path:
            self.df = pd.read_csv(data_path)
        else:
            raise ValueError("Cáº§n cung cáº¥p data_path hoáº·c df")

        self.processed_df = None
        self.X_train = None
        self.X_test = None
        self.y_train = None
        self.y_test = None
        self.models = {}
        self.results = {}

    def preprocess_data(self):
        """
        BÆ°á»›c 1: Tiá»n xá»­ lÃ½ dá»¯ liá»‡u
        - Log transform cÃ¡c biáº¿n (trá»« hci)
        - KNN Imputation cho missing values
        - Standardization
        """
        print("ğŸ”„ Báº¯t Ä‘áº§u tiá»n xá»­ lÃ½ dá»¯ liá»‡u...")

        # Táº¡o báº£n sao Ä‘á»ƒ xá»­ lÃ½
        df_processed = self.df.copy()

        # Äá»‹nh nghÄ©a cÃ¡c biáº¿n cáº§n xá»­ lÃ½
        meta_cols = ["Time", "Country Name", "Country Code"]
        numeric_cols = [col for col in df_processed.columns if col not in meta_cols]

        # 1.1 Log transform (trá»« hci vÃ¬ Ä‘Ã£ chuáº©n hÃ³a 0-1)
        log_cols = [col for col in numeric_cols if col != "hci"]

        for col in log_cols:
            # ThÃªm 1 Ä‘á»ƒ trÃ¡nh log(0), sau Ä‘Ã³ log transform
            df_processed[f"ln_{col}"] = np.log(df_processed[col] + 1)

        # Giá»¯ nguyÃªn hci
        df_processed["hci_scaled"] = df_processed["hci"]

        # Láº¥y cÃ¡c cá»™t Ä‘Ã£ transform
        feature_cols = [f"ln_{col}" for col in log_cols] + ["hci_scaled"]
        feature_data = df_processed[feature_cols]

        # 1.2 KNN Imputation cho missing values
        print("   ğŸ“Š Ãp dá»¥ng KNN Imputation...")
        knn_imputer = KNNImputer(n_neighbors=5)
        feature_data_imputed = knn_imputer.fit_transform(feature_data)
        feature_data_imputed = pd.DataFrame(
            feature_data_imputed, columns=feature_cols, index=feature_data.index
        )

        # 1.3 Standardization
        print("   ğŸ“ Chuáº©n hÃ³a dá»¯ liá»‡u...")
        scaler = StandardScaler()
        feature_data_scaled = scaler.fit_transform(feature_data_imputed)
        feature_data_scaled = pd.DataFrame(
            feature_data_scaled, columns=feature_cols, index=feature_data.index
        )

        # Gá»™p vá»›i metadata
        self.processed_df = pd.concat(
            [df_processed[meta_cols], feature_data_scaled], axis=1
        )
        self.scaler = scaler
        self.knn_imputer = knn_imputer

        print("âœ… HoÃ n thÃ nh tiá»n xá»­ lÃ½ dá»¯ liá»‡u!")
        return self.processed_df

    def create_composite_indices(self):
        """
        BÆ°á»›c 2: Táº¡o chá»‰ sá»‘ tá»•ng há»£p ICT vÃ  IC báº±ng PCA
        """
        print("ğŸ”„ Táº¡o chá»‰ sá»‘ tá»•ng há»£p ICT & IC...")

        if self.processed_df is None:
            raise ValueError("Cáº§n cháº¡y preprocess_data() trÆ°á»›c!")

        # 2.1 Táº¡o ICT Index tá»«: inet_usr, sec_srv, mob_sub
        ict_vars = ["ln_inet_usr", "ln_sec_srv", "ln_mob_sub"]
        ict_data = self.processed_df[ict_vars].dropna()

        pca_ict = PCA(n_components=1)
        ict_index = pca_ict.fit_transform(ict_data)

        # 2.2 Táº¡o IC Index tá»«: ter_enr, edu_exp, rnd_exp, sci_art, hci
        ic_vars = ["ln_ter_enr", "ln_edu_exp", "ln_rnd_exp", "ln_sci_art", "hci_scaled"]
        ic_data = self.processed_df[ic_vars].dropna()

        pca_ic = PCA(n_components=1)
        ic_index = pca_ic.fit_transform(ic_data)

        # GÃ¡n káº¿t quáº£ vá» dataframe chÃ­nh
        self.processed_df.loc[ict_data.index, "ICT_Index"] = ict_index.flatten()
        self.processed_df.loc[ic_data.index, "IC_Index"] = ic_index.flatten()

        # LÆ°u PCA models
        self.pca_ict = pca_ict
        self.pca_ic = pca_ic

        print(
            f"   ğŸ“ˆ ICT Index - Explained Variance: {pca_ict.explained_variance_ratio_[0]:.3f}"
        )
        print(
            f"   ğŸ“ˆ IC Index - Explained Variance: {pca_ic.explained_variance_ratio_[0]:.3f}"
        )
        print("âœ… HoÃ n thÃ nh táº¡o chá»‰ sá»‘ tá»•ng há»£p!")

    def cluster_countries(self, n_clusters=None):
        """
        BÆ°á»›c 3: PhÃ¢n cá»¥m quá»‘c gia báº±ng K-Means
        """
        print("ğŸ”„ PhÃ¢n cá»¥m quá»‘c gia...")

        # Dá»¯ liá»‡u Ä‘á»ƒ clustering
        cluster_vars = ["ICT_Index", "IC_Index", "ln_gdp", "ln_trade"]
        cluster_data = self.processed_df[cluster_vars].dropna()

        # TÃ¬m sá»‘ cá»¥m tá»‘i Æ°u náº¿u khÃ´ng Ä‘Æ°á»£c cung cáº¥p
        if n_clusters is None:
            inertias = []
            silhouette_scores = []
            K_range = range(2, 8)

            from sklearn.metrics import silhouette_score

            for k in K_range:
                kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
                kmeans.fit(cluster_data)
                inertias.append(kmeans.inertia_)
                silhouette_scores.append(silhouette_score(cluster_data, kmeans.labels_))

            # Chá»n k cÃ³ silhouette score cao nháº¥t
            optimal_k = K_range[np.argmax(silhouette_scores)]
            print(f"   ğŸ¯ Sá»‘ cá»¥m tá»‘i Æ°u: {optimal_k}")
        else:
            optimal_k = n_clusters

        # Thá»±c hiá»‡n clustering
        kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)
        cluster_labels = kmeans.fit_predict(cluster_data)

        # GÃ¡n nhÃ£n cá»¥m
        self.processed_df.loc[cluster_data.index, "cluster_label"] = cluster_labels

        self.kmeans = kmeans
        self.n_clusters = optimal_k

        print(f"âœ… HoÃ n thÃ nh phÃ¢n cá»¥m thÃ nh {optimal_k} nhÃ³m!")

    def prepare_modeling_data(self):
        """
        BÆ°á»›c 4: Chuáº©n bá»‹ dá»¯ liá»‡u cho modeling
        """
        print("ğŸ”„ Chuáº©n bá»‹ dá»¯ liá»‡u cho modeling...")

        # Biáº¿n phá»¥ thuá»™c (Y)
        target_vars = ["ln_gdp", "ln_fdi", "ln_hte", "ln_ict_exp"]

        # Biáº¿n Ä‘á»™c láº­p (X)
        feature_vars = [
            "ICT_Index",
            "IC_Index",
            "ln_pop",
            "ln_infl",
            "ln_trade",
            "ln_urb_area",
            "cluster_label",
        ]

        # Lá»c dá»¯ liá»‡u hoÃ n chá»‰nh
        all_vars = target_vars + feature_vars
        complete_data = self.processed_df[all_vars].dropna()

        # TÃ¡ch X vÃ  Y
        X = complete_data[feature_vars]
        y = complete_data[target_vars]

        # Chia train/test (80/20)
        self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )

        print(f"   ğŸ“Š KÃ­ch thÆ°á»›c train: {self.X_train.shape}")
        print(f"   ğŸ“Š KÃ­ch thÆ°á»›c test: {self.X_test.shape}")
        print("âœ… HoÃ n thÃ nh chuáº©n bá»‹ dá»¯ liá»‡u!")

    def train_models(self):
        """
        BÆ°á»›c 5: Huáº¥n luyá»‡n cÃ¡c mÃ´ hÃ¬nh ML
        """
        print("ğŸ”„ Huáº¥n luyá»‡n cÃ¡c mÃ´ hÃ¬nh ML...")

        # Khá»Ÿi táº¡o models
        models = {
            "LASSO": LassoCV(cv=5, random_state=42),
            "Random_Forest": RandomForestRegressor(n_estimators=100, random_state=42),
            "XGBoost": xgb.XGBRegressor(random_state=42, n_estimators=100),
            "Neural_Network": MLPRegressor(
                hidden_layer_sizes=(100, 50), max_iter=1000, random_state=42
            ),
        }

        # Huáº¥n luyá»‡n cho tá»«ng target variable
        for target in self.y_train.columns:
            print(f"   ğŸ¯ Huáº¥n luyá»‡n cho biáº¿n: {target}")
            self.models[target] = {}

            y_train_target = self.y_train[target]
            y_test_target = self.y_test[target]

            for model_name, model in models.items():
                # Huáº¥n luyá»‡n
                model.fit(self.X_train, y_train_target)

                # Dá»± Ä‘oÃ¡n
                y_pred_train = model.predict(self.X_train)
                y_pred_test = model.predict(self.X_test)

                # LÆ°u model vÃ  káº¿t quáº£
                self.models[target][model_name] = {
                    "model": model,
                    "y_pred_train": y_pred_train,
                    "y_pred_test": y_pred_test,
                    "r2_train": r2_score(y_train_target, y_pred_train),
                    "r2_test": r2_score(y_test_target, y_pred_test),
                    "rmse_test": np.sqrt(
                        mean_squared_error(y_test_target, y_pred_test)
                    ),
                    "mae_test": mean_absolute_error(y_test_target, y_pred_test),
                }

        print("âœ… HoÃ n thÃ nh huáº¥n luyá»‡n táº¥t cáº£ mÃ´ hÃ¬nh!")

    def evaluate_models(self):
        """
        BÆ°á»›c 6: ÄÃ¡nh giÃ¡ vÃ  so sÃ¡nh mÃ´ hÃ¬nh
        """
        print("ğŸ”„ ÄÃ¡nh giÃ¡ mÃ´ hÃ¬nh...")

        # Táº¡o báº£ng káº¿t quáº£
        results_list = []

        for target in self.models.keys():
            for model_name, model_info in self.models[target].items():
                results_list.append(
                    {
                        "Target": target,
                        "Model": model_name,
                        "R2_Train": model_info["r2_train"],
                        "R2_Test": model_info["r2_test"],
                        "RMSE_Test": model_info["rmse_test"],
                        "MAE_Test": model_info["mae_test"],
                    }
                )

        self.results_df = pd.DataFrame(results_list)

        print("ğŸ“Š Káº¿t quáº£ Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh:")
        print(self.results_df.round(4))

        return self.results_df

    def plot_results(self):
        """
        BÆ°á»›c 7: Trá»±c quan hÃ³a káº¿t quáº£
        """
        print("ğŸ”„ Táº¡o biá»ƒu Ä‘á»“ trá»±c quan...")

        # Táº¡o subplots
        fig, axes = plt.subplots(2, 2, figsize=(15, 12))
        fig.suptitle(
            "Káº¿t Quáº£ ÄÃ¡nh GiÃ¡ MÃ´ HÃ¬nh ML - ICT & IC Impact Analysis",
            fontsize=16,
            fontweight="bold",
        )

        # 1. So sÃ¡nh RÂ² Test cá»§a cÃ¡c mÃ´ hÃ¬nh
        ax1 = axes[0, 0]
        pivot_r2 = self.results_df.pivot(
            index="Model", columns="Target", values="R2_Test"
        )
        pivot_r2.plot(kind="bar", ax=ax1, width=0.8)
        ax1.set_title("RÂ² Score Comparison (Test Set)")
        ax1.set_ylabel("RÂ² Score")
        ax1.legend(bbox_to_anchor=(1.05, 1), loc="upper left")
        ax1.tick_params(axis="x", rotation=45)

        # 2. RMSE Comparison
        ax2 = axes[0, 1]
        pivot_rmse = self.results_df.pivot(
            index="Model", columns="Target", values="RMSE_Test"
        )
        pivot_rmse.plot(kind="bar", ax=ax2, width=0.8)
        ax2.set_title("RMSE Comparison (Test Set)")
        ax2.set_ylabel("RMSE")
        ax2.legend(bbox_to_anchor=(1.05, 1), loc="upper left")
        ax2.tick_params(axis="x", rotation=45)

        # 3. Feature Importance (Random Forest cho ln_gdp)
        ax3 = axes[1, 0]
        rf_model = self.models["ln_gdp"]["Random_Forest"]["model"]
        feature_importance = pd.DataFrame(
            {
                "Feature": self.X_train.columns,
                "Importance": rf_model.feature_importances_,
            }
        ).sort_values("Importance", ascending=False)

        sns.barplot(data=feature_importance, x="Importance", y="Feature", ax=ax3)
        ax3.set_title("Feature Importance (Random Forest - ln_gdp)")

        # 4. Clustering Visualization (2D PCA)
        ax4 = axes[1, 1]
        cluster_vars = ["ICT_Index", "IC_Index", "ln_gdp", "ln_trade"]
        cluster_data = self.processed_df[cluster_vars].dropna()

        pca_viz = PCA(n_components=2)
        cluster_pca = pca_viz.fit_transform(cluster_data)

        scatter = ax4.scatter(
            cluster_pca[:, 0],
            cluster_pca[:, 1],
            c=self.processed_df.loc[cluster_data.index, "cluster_label"],
            cmap="viridis",
            alpha=0.7,
        )
        ax4.set_title("Country Clusters (2D PCA Visualization)")
        ax4.set_xlabel(f"PC1 ({pca_viz.explained_variance_ratio_[0]:.2%} variance)")
        ax4.set_ylabel(f"PC2 ({pca_viz.explained_variance_ratio_[1]:.2%} variance)")
        plt.colorbar(scatter, ax=ax4, label="Cluster")

        plt.tight_layout()
        plt.show()

        # Feature Importance chi tiáº¿t cho táº¥t cáº£ targets
        self.plot_feature_importance()

    def plot_feature_importance(self):
        """
        Biá»ƒu Ä‘á»“ Feature Importance chi tiáº¿t
        """
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle(
            "Feature Importance Analysis (Random Forest)",
            fontsize=16,
            fontweight="bold",
        )

        targets = list(self.models.keys())

        for i, target in enumerate(targets):
            ax = axes[i // 2, i % 2]

            rf_model = self.models[target]["Random_Forest"]["model"]
            feature_importance = pd.DataFrame(
                {
                    "Feature": self.X_train.columns,
                    "Importance": rf_model.feature_importances_,
                }
            ).sort_values("Importance", ascending=False)

            sns.barplot(data=feature_importance, x="Importance", y="Feature", ax=ax)
            ax.set_title(f"Feature Importance - {target}")
            ax.set_xlabel("Importance Score")

        plt.tight_layout()
        plt.show()

    def get_summary_report(self):
        """
        Táº¡o bÃ¡o cÃ¡o tá»•ng káº¿t
        """
        print("\n" + "=" * 50)
        print("ğŸ“‹ BÃO CÃO Tá»”NG Káº¾T PHÃ‚N TÃCH")
        print("=" * 50)

        print(f"ğŸ“Š KÃ­ch thÆ°á»›c dá»¯ liá»‡u: {self.processed_df.shape}")
        print(f"ğŸ¯ Sá»‘ cá»¥m quá»‘c gia: {self.n_clusters}")
        print(f"ğŸ§  Sá»‘ mÃ´ hÃ¬nh Ä‘Æ°á»£c huáº¥n luyá»‡n: {len(self.models) * 4}")

        print("\nğŸ† MÃ” HÃŒNH Tá»T NHáº¤T CHO Tá»ªNG BIáº¾N:")
        for target in self.models.keys():
            best_model = max(
                self.models[target].keys(),
                key=lambda x: self.models[target][x]["r2_test"],
            )
            best_r2 = self.models[target][best_model]["r2_test"]
            print(f"   {target}: {best_model} (RÂ² = {best_r2:.3f})")

        print("\nğŸ’¡ INSIGHTS CHÃNH:")
        print("   ğŸ”¹ ICT vÃ  IC Index Ä‘Ã£ Ä‘Æ°á»£c táº¡o thÃ nh cÃ´ng báº±ng PCA")
        print("   ğŸ”¹ CÃ¡c quá»‘c gia Ä‘Æ°á»£c phÃ¢n cá»¥m dá»±a trÃªn má»©c Ä‘á»™ phÃ¡t triá»ƒn")
        print("   ğŸ”¹ MÃ´ hÃ¬nh ML cho tháº¥y tÃ¡c Ä‘á»™ng Ä‘Ã¡ng ká»ƒ cá»§a ICT & IC")

        return self.results_df


# HÃ m chÃ­nh Ä‘á»ƒ cháº¡y toÃ n bá»™ pipeline
def run_complete_pipeline(data_path=None, df=None):
    """
    Cháº¡y toÃ n bá»™ pipeline phÃ¢n tÃ­ch

    Parameters:
    - data_path: Ä‘Æ°á»ng dáº«n file CSV
    - df: DataFrame náº¿u Ä‘Ã£ load sáºµn

    Returns:
    - pipeline: object chá»©a toÃ n bá»™ káº¿t quáº£
    """
    # Khá»Ÿi táº¡o pipeline
    pipeline = ICT_IC_MLPipeline(data_path=data_path, df=df)

    # Cháº¡y tá»«ng bÆ°á»›c
    pipeline.preprocess_data()
    pipeline.create_composite_indices()
    pipeline.cluster_countries()
    pipeline.prepare_modeling_data()
    pipeline.train_models()
    pipeline.evaluate_models()
    pipeline.plot_results()
    pipeline.get_summary_report()

    return pipeline


# =====================================
# CHáº Y PIPELINE Vá»šI FILE "rawdata_1"
# =====================================


def main():
    """
    HÃ m chÃ­nh Ä‘á»ƒ cháº¡y pipeline vá»›i file rawdata_1
    """
    try:
        print("ğŸš€ Báº®T Äáº¦U CHáº Y PIPELINE PHÃ‚N TÃCH ICT & IC")
        print("=" * 60)

        # Cháº¡y pipeline vá»›i file rawdata_1
        pipeline = run_complete_pipeline(data_path="rawdata_1.csv")

        print("\nğŸŠ HOÃ€N THÃ€NH Táº¤T Cáº¢ CÃC BÆ¯á»šC!")
        print("=" * 60)
        print("ğŸ“ Káº¿t quáº£ Ä‘Ã£ Ä‘Æ°á»£c lÆ°u trong object 'pipeline'")
        print("ğŸ“Š Äá»ƒ xem báº£ng káº¿t quáº£: pipeline.results_df")
        print("ğŸ¤– Äá»ƒ truy cáº­p mÃ´ hÃ¬nh: pipeline.models")
        print("ğŸ“ˆ Äá»ƒ xem dá»¯ liá»‡u Ä‘Ã£ xá»­ lÃ½: pipeline.processed_df")

        return pipeline

    except FileNotFoundError:
        print("âŒ KhÃ´ng tÃ¬m tháº¥y file 'rawdata_1.csv'")
        print("ğŸ“‹ Vui lÃ²ng kiá»ƒm tra:")
        print("   - File cÃ³ tá»“n táº¡i khÃ´ng?")
        print("   - TÃªn file cÃ³ Ä‘Ãºng khÃ´ng? (rawdata_1.csv)")
        print("   - File cÃ³ á»Ÿ cÃ¹ng thÆ° má»¥c vá»›i code khÃ´ng?")
        return None

    except Exception as e:
        print(f"âŒ Lá»—i khi cháº¡y pipeline: {str(e)}")
        print("ğŸ“ Vui lÃ²ng kiá»ƒm tra format dá»¯ liá»‡u vÃ  thá»­ láº¡i")
        return None


# =====================================
# CÃCH Sá»¬ Dá»¤NG KHÃC:
# =====================================

# Option 1: Cháº¡y vá»›i file rawdata_1 (KHUYáº¾N NGHá»Š)
# pipeline = main()

# Option 2: Cháº¡y trá»±c tiáº¿p
# pipeline = run_complete_pipeline(data_path='rawdata_1.csv')

# Option 3: Tá»« DataFrame Ä‘Ã£ cÃ³
# pipeline = run_complete_pipeline(df=your_dataframe)

# =====================================
# TRUY XUáº¤T Káº¾T QUáº¢:
# =====================================

# ğŸ“Š Xem báº£ng Ä‘Ã¡nh giÃ¡ mÃ´ hÃ¬nh:
# print(pipeline.results_df)

# ğŸ† TÃ¬m mÃ´ hÃ¬nh tá»‘t nháº¥t cho ln_gdp:
# gdp_results = pipeline.results_df[pipeline.results_df['Target'] == 'ln_gdp']
# best_model = gdp_results.loc[gdp_results['R2_Test'].idxmax()]
# print(f"MÃ´ hÃ¬nh tá»‘t nháº¥t cho GDP: {best_model['Model']} vá»›i RÂ² = {best_model['R2_Test']:.3f}")

# ğŸ“ˆ Xem feature importance cho Random Forest:
# rf_model = pipeline.models['ln_gdp']['Random_Forest']['model']
# feature_names = pipeline.X_train.columns
# importance_df = pd.DataFrame({
#     'Feature': feature_names,
#     'Importance': rf_model.feature_importances_
# }).sort_values('Importance', ascending=False)
# print(importance_df)

# ğŸ—ºï¸ Xem thÃ´ng tin clustering:
# cluster_info = pipeline.processed_df['cluster_label'].value_counts().sort_index()
# print("Sá»‘ quá»‘c gia trong má»—i cá»¥m:")
# print(cluster_info)

print("ğŸ‰ PIPELINE Sáº´N SÃ€NG CHáº Y Vá»šI FILE 'rawdata_1.csv'!")
print("ğŸš€ Äá»ƒ báº¯t Ä‘áº§u, cháº¡y lá»‡nh: pipeline = main()")
print(
    "âš¡ Hoáº·c cháº¡y trá»±c tiáº¿p: pipeline = run_complete_pipeline(data_path='rawdata_1.csv')"
)
