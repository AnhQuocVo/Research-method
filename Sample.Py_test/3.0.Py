import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.model_selection import train_test_split
from sklearn.metrics import r2_score, mean_squared_error
from linearmodels.panel import PanelOLS, PooledOLS, RandomEffects
import warnings

warnings.filterwarnings("ignore")

# Thiáº¿t láº­p style cho plots
plt.style.use("seaborn-v0_8")
sns.set_palette("husl")


class ICTIntellectualCapitalAnalysis:
    def __init__(self, file_path):
        """
        Khá»Ÿi táº¡o class phÃ¢n tÃ­ch
        """
        self.file_path = file_path
        self.df = None
        self.df_processed = None
        self.clusters_df = None
        self.model_results = {}

    def load_and_explore_data(self):
        """
        1. Load vÃ  khÃ¡m phÃ¡ dá»¯ liá»‡u ban Ä‘áº§u
        """
        print("=" * 60)
        print("1. LOADING VÃ€ KHÃM PHÃ Dá»® LIá»†U")
        print("=" * 60)

        # Load data
        if self.file_path.endswith(".csv"):
            self.df = pd.read_csv(
                r"C:\Users\VAQ\OneDrive - TRÆ¯á»œNG Äáº I Há»ŒC Má» TP.HCM\University\Research paper\RP_2 ngÆ°Æ¡i báº¡n\Data\ML Process\Data_csv\rawdata_1.csv"
            )
        else:
            self.df = pd.read_excel(self.file_path)

        print(f"KÃ­ch thÆ°á»›c dataset: {self.df.shape}")
        print(f"Sá»‘ quá»‘c gia: {self.df['Country Name'].nunique()}")
        print(f"Thá»i gian: {self.df['Time'].min()} - {self.df['Time'].max()}")

        # Kiá»ƒm tra missing values
        print("\nMissing values theo biáº¿n:")
        missing_data = self.df.isnull().sum()
        missing_percent = (missing_data / len(self.df)) * 100
        missing_df = pd.DataFrame(
            {"Missing Count": missing_data, "Percentage": missing_percent}
        ).sort_values("Percentage", ascending=False)
        print(missing_df[missing_df["Missing Count"] > 0])

        # Thá»‘ng kÃª mÃ´ táº£
        print("\nThá»‘ng kÃª mÃ´ táº£ cÃ¡c biáº¿n chÃ­nh:")
        key_vars = [
            "gdp",
            "inet_usr",
            "sec_srv",
            "mob_sub",
            "ter_enr",
            "edu_exp",
            "rnd_exp",
            "sci_art",
            "hci",
            "pop",
            "infl",
            "urb_area",
            "trade",
        ]
        print(self.df[key_vars].describe())

        return self.df

    def process_data(self):
        """
        2. Xá»­ lÃ½ vÃ  chuáº©n bá»‹ dá»¯ liá»‡u
        """
        print("\n\n" + "=" * 60)
        print("2. Xá»¬ LÃ VÃ€ CHUáº¨N Bá»Š Dá»® LIá»†U")
        print("=" * 60)

        # Táº¡o báº£n copy Ä‘á»ƒ xá»­ lÃ½
        df_work = self.df.copy()

        # Äá»‹nh nghÄ©a cÃ¡c biáº¿n cáº§n thiáº¿t cho mÃ´ hÃ¬nh
        model_vars = [
            "Time",
            "Country Name",
            "Country Code",
            "gdp",
            "inet_usr",
            "sec_srv",
            "mob_sub",
            "ter_enr",  # ICT variables
            "edu_exp",
            "rnd_exp",
            "sci_art",
            "hci",  # IC variables
            "pop",
            "infl",
            "urb_area",
            "trade",
        ]  # Control variables

        # Giá»¯ chá»‰ cÃ¡c biáº¿n cáº§n thiáº¿t
        df_work = df_work[model_vars]

        print(f"Dá»¯ liá»‡u trÆ°á»›c khi drop missing: {df_work.shape}")

        # Drop táº¥t cáº£ rows cÃ³ missing values
        df_work = df_work.dropna()
        print(f"Dá»¯ liá»‡u sau khi drop missing: {df_work.shape}")

        # Log transformation cho cÃ¡c biáº¿n phÃ¹ há»£p (giÃ¡ trá»‹ > 0)
        log_vars = ["gdp", "inet_usr", "sec_srv", "ter_enr", "pop", "trade"]

        for var in log_vars:
            if var in df_work.columns:
                # Chá»‰ log transform cÃ¡c giÃ¡ trá»‹ > 0
                mask = df_work[var] > 0
                df_work.loc[mask, f"ln_{var}"] = np.log(df_work.loc[mask, var])
                print(f"Log transformed {var}: {mask.sum()} observations")

        # Táº¡o ICT Index (chuáº©n hÃ³a rá»“i tÃ­nh trung bÃ¬nh)
        ict_vars = ["ln_inet_usr", "ln_sec_srv", "mob_sub", "ln_ter_enr"]

        # Chuáº©n hÃ³a cÃ¡c biáº¿n ICT
        scaler = StandardScaler()
        ict_data = df_work[ict_vars].copy()
        ict_data = ict_data.dropna()

        if len(ict_data) > 0:
            ict_scaled = scaler.fit_transform(ict_data)
            df_work.loc[ict_data.index, "ict_index"] = np.mean(ict_scaled, axis=1)
            print(f"Táº¡o ICT Index thÃ nh cÃ´ng cho {len(ict_data)} observations")

        # TÃ­nh GDP per capita
        df_work["gdp_pc"] = df_work["gdp"] / df_work["pop"]
        df_work["ln_gdp_pc"] = np.log(df_work["gdp_pc"])

        # Drop nhá»¯ng dÃ²ng váº«n cÃ²n missing sau khi táº¡o biáº¿n má»›i
        df_work = df_work.dropna()

        print(f"Dá»¯ liá»‡u cuá»‘i cÃ¹ng: {df_work.shape}")
        print(f"Thá»i gian: {df_work['Time'].min()} - {df_work['Time'].max()}")
        print(f"Sá»‘ quá»‘c gia: {df_work['Country Name'].nunique()}")

        self.df_processed = df_work
        return df_work

    def cluster_analysis(self):
        """
        3. PhÃ¢n nhÃ³m quá»‘c gia báº±ng clustering
        """
        print("\n\n" + "=" * 60)
        print("3. PHÃ‚N NHÃ“M QUá»C GIA (CLUSTERING)")
        print("=" * 60)

        # Chuáº©n bá»‹ dá»¯ liá»‡u cho clustering (láº¥y giÃ¡ trá»‹ trung bÃ¬nh theo quá»‘c gia)
        cluster_vars = ["gdp_pc", "hci", "ict_index"]
        cluster_data = (
            self.df_processed.groupby("Country Name")[cluster_vars].mean().reset_index()
        )
        cluster_data = cluster_data.dropna()

        print(f"Sá»‘ quá»‘c gia Ä‘á»ƒ clustering: {len(cluster_data)}")

        # Chuáº©n hÃ³a dá»¯ liá»‡u
        scaler = StandardScaler()
        X_scaled = scaler.fit_transform(cluster_data[cluster_vars])

        # TÃ¬m sá»‘ cluster tá»‘i Æ°u báº±ng silhouette score
        silhouette_scores = []
        K_range = range(2, min(8, len(cluster_data) // 2))

        for k in K_range:
            kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)
            labels = kmeans.fit_predict(X_scaled)
            score = silhouette_score(X_scaled, labels)
            silhouette_scores.append(score)

        # Chá»n sá»‘ cluster tá»‘i Æ°u
        optimal_k = K_range[np.argmax(silhouette_scores)]
        print(
            f"Sá»‘ cluster tá»‘i Æ°u: {optimal_k} (Silhouette Score: {max(silhouette_scores):.3f})"
        )

        # Thá»±c hiá»‡n clustering vá»›i K tá»‘i Æ°u
        kmeans_final = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)
        cluster_data["Cluster"] = kmeans_final.fit_predict(X_scaled)

        # PhÃ¢n tÃ­ch Ä‘áº·c Ä‘iá»ƒm cÃ¡c cluster
        print("\nÄáº·c Ä‘iá»ƒm cÃ¡c cluster:")
        cluster_summary = cluster_data.groupby("Cluster")[cluster_vars].agg(
            ["mean", "std", "count"]
        )
        print(cluster_summary)

        # Hiá»ƒn thá»‹ má»™t sá»‘ quá»‘c gia tiÃªu biá»ƒu trong má»—i cluster
        print("\nMá»™t sá»‘ quá»‘c gia tiÃªu biá»ƒu trong má»—i cluster:")
        for cluster_id in sorted(cluster_data["Cluster"].unique()):
            countries = cluster_data[cluster_data["Cluster"] == cluster_id][
                "Country Name"
            ].tolist()
            print(
                f"Cluster {cluster_id}: {', '.join(countries[:5])}{'...' if len(countries) > 5 else ''}"
            )

        self.clusters_df = cluster_data

        # Merge cluster info back to main dataset
        self.df_processed = self.df_processed.merge(
            cluster_data[["Country Name", "Cluster"]], on="Country Name", how="left"
        )

        return cluster_data

    def run_econometric_models(self):
        """
        4. Cháº¡y cÃ¡c mÃ´ hÃ¬nh kinh táº¿ lÆ°á»£ng
        """
        print("\n\n" + "=" * 60)
        print("4. MÃ” HÃŒNH KINH Táº¾ LÆ¯á»¢NG")
        print("=" * 60)

        # Chuáº©n bá»‹ dá»¯ liá»‡u cho panel regression
        df_panel = self.df_processed.copy()
        df_panel = df_panel.set_index(["Country Name", "Time"])

        # Äá»‹nh nghÄ©a biáº¿n phá»¥ thuá»™c vÃ  biáº¿n Ä‘á»™c láº­p
        y_var = "ln_gdp"

        # Biáº¿n Ä‘á»™c láº­p
        X_vars = [
            "ln_inet_usr",
            "ln_sec_srv",
            "mob_sub",
            "ln_ter_enr",  # ICT vars
            "edu_exp",
            "rnd_exp",
            "sci_art",
            "hci",  # IC vars
            "ln_pop",
            "infl",
            "urb_area",
            "ln_trade",  # Control vars
        ]

        # Lá»c cÃ¡c biáº¿n thá»±c sá»± cÃ³ trong data
        available_X_vars = [
            var
            for var in X_vars
            if var in df_panel.columns and not df_panel[var].isnull().all()
        ]
        print(f"Biáº¿n Ä‘á»™c láº­p sá»­ dá»¥ng: {available_X_vars}")

        df_model = df_panel[[y_var] + available_X_vars].dropna()
        print(f"Sá»‘ observations cho mÃ´ hÃ¬nh: {len(df_model)}")

        if len(df_model) == 0:
            print("KhÃ´ng cÃ³ dá»¯ liá»‡u Ä‘á»ƒ cháº¡y mÃ´ hÃ¬nh!")
            return None

        # 1. Pooled OLS
        print("\n--- Pooled OLS ---")
        try:
            pooled_model = PooledOLS(df_model[y_var], df_model[available_X_vars])
            pooled_results = pooled_model.fit(cov_type="robust")
            print(pooled_results.summary)
            self.model_results["pooled"] = pooled_results
        except Exception as e:
            print(f"Lá»—i Pooled OLS: {e}")

        # 2. Fixed Effects
        print("\n--- Fixed Effects ---")
        try:
            fe_model = PanelOLS(
                df_model[y_var], df_model[available_X_vars], entity_effects=True
            )
            fe_results = fe_model.fit(cov_type="robust")
            print(fe_results.summary)
            self.model_results["fixed_effects"] = fe_results
        except Exception as e:
            print(f"Lá»—i Fixed Effects: {e}")

        # 3. Random Effects
        print("\n--- Random Effects ---")
        try:
            re_model = RandomEffects(df_model[y_var], df_model[available_X_vars])
            re_results = re_model.fit(cov_type="robust")
            print(re_results.summary)
            self.model_results["random_effects"] = re_results
        except Exception as e:
            print(f"Lá»—i Random Effects: {e}")

        # So sÃ¡nh R-squared
        print("\n--- So sÃ¡nh mÃ´ hÃ¬nh ---")
        for model_name, results in self.model_results.items():
            try:
                r2 = (
                    results.rsquared
                    if hasattr(results, "rsquared")
                    else results.rsquared_overall
                )
                print(f"{model_name}: RÂ² = {r2:.4f}")
            except Exception:
                print(f"{model_name}: KhÃ´ng tÃ­nh Ä‘Æ°á»£c RÂ²")

        return self.model_results

    def create_visualizations(self):
        """
        5. Táº¡o trá»±c quan hÃ³a
        """
        print("\n\n" + "=" * 60)
        print("5. TRá»°C QUAN HÃ“A")
        print("=" * 60)

        # Setup figure
        plt.figure(figsize=(20, 15))

        # 1. Heatmap correlation matrix
        plt.subplot(2, 3, 1)
        corr_vars = [
            "ln_gdp",
            "ln_inet_usr",
            "ln_sec_srv",
            "mob_sub",
            "ln_ter_enr",
            "edu_exp",
            "rnd_exp",
            "sci_art",
            "hci",
            "ln_pop",
            "infl",
            "urb_area",
            "ln_trade",
        ]
        available_corr_vars = [
            var for var in corr_vars if var in self.df_processed.columns
        ]

        corr_matrix = self.df_processed[available_corr_vars].corr()
        sns.heatmap(
            corr_matrix,
            annot=True,
            cmap="coolwarm",
            center=0,
            square=True,
            fmt=".2f",
            cbar_kws={"shrink": 0.8},
        )
        plt.title("Ma tráº­n tÆ°Æ¡ng quan cÃ¡c biáº¿n", fontsize=12, fontweight="bold")
        plt.xticks(rotation=45)
        plt.yticks(rotation=0)

        # 2. Cluster visualization (náº¿u cÃ³ Ä‘á»§ data)
        if self.clusters_df is not None and len(self.clusters_df) > 0:
            plt.subplot(2, 3, 2)
            scatter = plt.scatter(
                self.clusters_df["gdp_pc"],
                self.clusters_df["hci"],
                c=self.clusters_df["Cluster"],
                cmap="viridis",
                s=100,
                alpha=0.7,
            )
            plt.colorbar(scatter)
            plt.xlabel("GDP per capita", fontweight="bold")
            plt.ylabel("Human Capital Index", fontweight="bold")
            plt.title("PhÃ¢n cá»¥m quá»‘c gia\n(GDP per capita vs HCI)", fontweight="bold")
            plt.grid(True, alpha=0.3)

        # 3. GDP distribution by cluster
        if "Cluster" in self.df_processed.columns:
            plt.subplot(2, 3, 3)
            for cluster in sorted(self.df_processed["Cluster"].unique()):
                cluster_data = self.df_processed[
                    self.df_processed["Cluster"] == cluster
                ]
                plt.hist(
                    cluster_data["ln_gdp"],
                    alpha=0.6,
                    label=f"Cluster {cluster}",
                    bins=20,
                )
            plt.xlabel("ln(GDP)", fontweight="bold")
            plt.ylabel("Frequency", fontweight="bold")
            plt.title("PhÃ¢n bá»‘ GDP theo cluster", fontweight="bold")
            plt.legend()
            plt.grid(True, alpha=0.3)

        # 4. ICT Index vs GDP
        plt.subplot(2, 3, 4)
        if "ict_index" in self.df_processed.columns:
            plt.scatter(
                self.df_processed["ict_index"],
                self.df_processed["ln_gdp"],
                alpha=0.6,
                color="blue",
            )
            plt.xlabel("ICT Index", fontweight="bold")
            plt.ylabel("ln(GDP)", fontweight="bold")
            plt.title("ICT Index vs GDP", fontweight="bold")
            plt.grid(True, alpha=0.3)

            # Add trend line
            z = np.polyfit(
                self.df_processed["ict_index"].dropna(),
                self.df_processed.loc[
                    self.df_processed["ict_index"].dropna().index, "ln_gdp"
                ],
                1,
            )
            p = np.poly1d(z)
            plt.plot(
                sorted(self.df_processed["ict_index"].dropna()),
                p(sorted(self.df_processed["ict_index"].dropna())),
                "r--",
                alpha=0.8,
            )

        # 5. Time series cá»§a má»™t sá»‘ quá»‘c gia
        plt.subplot(2, 3, 5)
        top_countries = (
            self.df_processed.groupby("Country Name")["gdp"].mean().nlargest(5).index
        )
        for country in top_countries:
            country_data = self.df_processed[
                self.df_processed["Country Name"] == country
            ]
            if len(country_data) > 1:
                plt.plot(
                    country_data["Time"],
                    country_data["ln_gdp"],
                    marker="o",
                    label=country,
                    linewidth=2,
                )
        plt.xlabel("Year", fontweight="bold")
        plt.ylabel("ln(GDP)", fontweight="bold")
        plt.title("Xu hÆ°á»›ng GDP cá»§a cÃ¡c quá»‘c gia hÃ ng Ä‘áº§u", fontweight="bold")
        plt.legend(bbox_to_anchor=(1.05, 1), loc="upper left")
        plt.grid(True, alpha=0.3)

        # 6. Human Capital vs R&D
        plt.subplot(2, 3, 6)
        plt.scatter(
            self.df_processed["hci"],
            self.df_processed["rnd_exp"],
            alpha=0.6,
            color="green",
        )
        plt.xlabel("Human Capital Index", fontweight="bold")
        plt.ylabel("R&D Expenditure (% GDP)", fontweight="bold")
        plt.title("Human Capital vs R&D Investment", fontweight="bold")
        plt.grid(True, alpha=0.3)

        plt.tight_layout()
        plt.show()

        print("CÃ¡c biá»ƒu Ä‘á»“ Ä‘Ã£ Ä‘Æ°á»£c táº¡o thÃ nh cÃ´ng!")

    def machine_learning_analysis(self):
        """
        6. PhÃ¢n tÃ­ch Machine Learning
        """
        print("\n\n" + "=" * 60)
        print("6. PHÃ‚N TÃCH MACHINE LEARNING")
        print("=" * 60)

        # Chuáº©n bá»‹ dá»¯ liá»‡u
        ml_vars = [
            "ln_inet_usr",
            "ln_sec_srv",
            "mob_sub",
            "ln_ter_enr",
            "edu_exp",
            "rnd_exp",
            "sci_art",
            "hci",
            "ln_pop",
            "infl",
            "urb_area",
            "ln_trade",
        ]

        available_ml_vars = [var for var in ml_vars if var in self.df_processed.columns]

        df_ml = self.df_processed[["ln_gdp"] + available_ml_vars].dropna()

        if len(df_ml) == 0:
            print("KhÃ´ng cÃ³ Ä‘á»§ dá»¯ liá»‡u cho ML analysis!")
            return None

        X = df_ml[available_ml_vars]
        y = df_ml["ln_gdp"]

        # Chia data
        X_train, X_test, y_train, y_test = train_test_split(
            X, y, test_size=0.2, random_state=42
        )

        print(f"Training set: {X_train.shape}")
        print(f"Test set: {X_test.shape}")

        # Random Forest
        print("\n--- Random Forest ---")
        rf_model = RandomForestRegressor(n_estimators=100, random_state=42)
        rf_model.fit(X_train, y_train)

        rf_pred = rf_model.predict(X_test)
        rf_r2 = r2_score(y_test, rf_pred)
        rf_rmse = np.sqrt(mean_squared_error(y_test, rf_pred))

        print(f"Random Forest RÂ²: {rf_r2:.4f}")
        print(f"Random Forest RMSE: {rf_rmse:.4f}")

        # Feature importance
        feature_importance = pd.DataFrame(
            {"Variable": available_ml_vars, "Importance": rf_model.feature_importances_}
        ).sort_values("Importance", ascending=False)

        print("\nFeature Importance (Random Forest):")
        print(feature_importance)

        # Gradient Boosting
        print("\n--- Gradient Boosting ---")
        gb_model = GradientBoostingRegressor(n_estimators=100, random_state=42)
        gb_model.fit(X_train, y_train)

        gb_pred = gb_model.predict(X_test)
        gb_r2 = r2_score(y_test, gb_pred)
        gb_rmse = np.sqrt(mean_squared_error(y_test, gb_pred))

        print(f"Gradient Boosting RÂ²: {gb_r2:.4f}")
        print(f"Gradient Boosting RMSE: {gb_rmse:.4f}")

        # Feature importance cho GB
        gb_feature_importance = pd.DataFrame(
            {"Variable": available_ml_vars, "Importance": gb_model.feature_importances_}
        ).sort_values("Importance", ascending=False)

        print("\nFeature Importance (Gradient Boosting):")
        print(gb_feature_importance)

        # Visualize feature importance
        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

        # Random Forest
        ax1.barh(
            feature_importance["Variable"][:10], feature_importance["Importance"][:10]
        )
        ax1.set_title("Random Forest Feature Importance", fontweight="bold")
        ax1.set_xlabel("Importance", fontweight="bold")

        # Gradient Boosting
        ax2.barh(
            gb_feature_importance["Variable"][:10],
            gb_feature_importance["Importance"][:10],
        )
        ax2.set_title("Gradient Boosting Feature Importance", fontweight="bold")
        ax2.set_xlabel("Importance", fontweight="bold")

        plt.tight_layout()
        plt.show()

        return {
            "random_forest": {
                "model": rf_model,
                "r2": rf_r2,
                "rmse": rf_rmse,
                "importance": feature_importance,
            },
            "gradient_boosting": {
                "model": gb_model,
                "r2": gb_r2,
                "rmse": gb_rmse,
                "importance": gb_feature_importance,
            },
        }

    def generate_final_report(self):
        """
        7. Táº¡o bÃ¡o cÃ¡o tá»•ng há»£p vÃ  gá»£i Ã½
        """
        print("\n\n" + "=" * 80)
        print("7. BÃO CÃO Tá»”NG Há»¢P VÃ€ Gá»¢I Ã")
        print("=" * 80)

        print("\nğŸ” Tá»”NG QUAN NGHIÃŠN Cá»©U:")
        print("â”€" * 50)
        print(f"â€¢ Dataset cuá»‘i cÃ¹ng: {self.df_processed.shape}")
        print(f"â€¢ Sá»‘ quá»‘c gia: {self.df_processed['Country Name'].nunique()}")
        print(
            f"â€¢ Khoáº£ng thá»i gian: {self.df_processed['Time'].min()} - {self.df_processed['Time'].max()}"
        )

        if self.clusters_df is not None:
            print(f"â€¢ Sá»‘ cluster quá»‘c gia: {self.clusters_df['Cluster'].nunique()}")

        print("\nğŸ“Š Káº¾T QUáº¢ MÃ” HÃŒNH KINH Táº¾ LÆ¯á»¢NG:")
        print("â”€" * 50)

        if self.model_results:
            best_model = None
            best_r2 = -1

            for model_name, results in self.model_results.items():
                try:
                    r2 = (
                        results.rsquared
                        if hasattr(results, "rsquared")
                        else results.rsquared_overall
                    )
                    print(f"â€¢ {model_name.replace('_', ' ').title()}: RÂ² = {r2:.4f}")

                    if r2 > best_r2:
                        best_r2 = r2
                        best_model = model_name
                except Exception:
                    print(
                        f"â€¢ {model_name.replace('_', ' ').title()}: KhÃ´ng tÃ­nh Ä‘Æ°á»£c RÂ²"
                    )

            if best_model:
                print(
                    f"\nâœ… MÃ” HÃŒNH ÄÆ¯á»¢C Äá»€ XUáº¤T: {best_model.replace('_', ' ').title()}"
                )
                print(f"   RÂ² = {best_r2:.4f}")

        print("\nğŸ’¡ GIáº¢I THÃCH Ã NGHÄ¨A CÃC BIáº¾N:")
        print("â”€" * 50)
        print("ğŸ“± ICT Variables:")
        print("   â€¢ ln_inet_usr: Internet users - pháº£n Ã¡nh kháº£ nÄƒng tiáº¿p cáº­n thÃ´ng tin")
        print("   â€¢ ln_sec_srv: Secure internet servers - Ä‘o lÆ°á»ng háº¡ táº§ng ICT an toÃ n")
        print("   â€¢ mob_sub: Mobile subscriptions - má»©c Ä‘á»™ phá»• biáº¿n cÃ´ng nghá»‡ di Ä‘á»™ng")
        print("   â€¢ ln_ter_enr: Tertiary enrollment - giÃ¡o dá»¥c Ä‘áº¡i há»c, ná»n táº£ng ICT")

        print("\nğŸ§  Intellectual Capital Variables:")
        print("   â€¢ edu_exp: Education expenditure - Ä‘áº§u tÆ° vÃ o vá»‘n nhÃ¢n lá»±c")
        print("   â€¢ rnd_exp: R&D expenditure - Ä‘áº§u tÆ° nghiÃªn cá»©u phÃ¡t triá»ƒn")
        print("   â€¢ sci_art: Scientific articles - Ä‘áº§u ra nghiÃªn cá»©u khoa há»c")
        print("   â€¢ hci: Human capital index - chá»‰ sá»‘ vá»‘n nhÃ¢n lá»±c tá»•ng há»£p")

        print("\nğŸ¯ Gá»¢I Ã CHÃNH SÃCH:")
        print("â”€" * 50)
        print("1. ğŸš€ Äáº§u tÆ° ICT: PhÃ¡t triá»ƒn háº¡ táº§ng internet vÃ  cÃ´ng nghá»‡ sá»‘")
        print("2. ğŸ“š GiÃ¡o dá»¥c: TÄƒng chi tiÃªu giÃ¡o dá»¥c, Ä‘áº·c biá»‡t giÃ¡o dá»¥c Ä‘áº¡i há»c")
        print("3. ğŸ”¬ R&D: Khuyáº¿n khÃ­ch Ä‘áº§u tÆ° nghiÃªn cá»©u phÃ¡t triá»ƒn")
        print("4. ğŸ‘¥ Vá»‘n nhÃ¢n lá»±c: PhÃ¡t triá»ƒn ká»¹ nÄƒng sá»‘ vÃ  nÄƒng lá»±c sÃ¡ng táº¡o")

        print("\nğŸ“ˆ Gá»¢I Ã Cáº¢I THIá»†N NGHIÃŠN Cá»¨U:")
        print("â”€" * 50)
        print("1. ğŸ“Š Dá»¯ liá»‡u: Thu tháº­p thÃªm dá»¯ liá»‡u vá»:")
        print("   â€¢ Cháº¥t lÆ°á»£ng ICT (tá»‘c Ä‘á»™ internet, Ä‘á»™ tin cáº­y)")
        print("   â€¢ Innovation metrics (patents, startups)")
        print("   â€¢ Digital skills cá»§a lao Ä‘á»™ng")

        print("2. ğŸ”§ MÃ´ hÃ¬nh: Xem xÃ©t:")
        print("   â€¢ Nonlinear relationships")
        print("   â€¢ Interaction effects giá»¯a ICT vÃ  IC")
        print("   â€¢ Dynamic panel models")
        print("   â€¢ Threshold effects")

        print("3. ğŸŒ PhÃ¢n tÃ­ch: Má»Ÿ rá»™ng:")
        print("   â€¢ So sÃ¡nh theo nhÃ³m quá»‘c gia (developed vs developing)")
        print("   â€¢ PhÃ¢n tÃ­ch theo ngÃ nh kinh táº¿")
        print("   â€¢ Time-varying coefficients")

        print("\n" + "=" * 80)
        print("âœ… PHÃ‚N TÃCH HOÃ€N Táº¤T!")
        print("=" * 80)


def main():
    """
    HÃ m main Ä‘á»ƒ cháº¡y toÃ n bá»™ phÃ¢n tÃ­ch
    """
    # Thay Ä‘á»•i Ä‘Æ°á»ng dáº«n file cá»§a báº¡n á»Ÿ Ä‘Ã¢y
    file_path = "your_dataset.xlsx"  # hoáº·c "your_dataset.csv"

    print("ğŸš€ Báº®T Äáº¦U PHÃ‚N TÃCH ICT VÃ€ INTELLECTUAL CAPITAL")
    print("=" * 80)

    # Khá»Ÿi táº¡o analyzer
    analyzer = ICTIntellectualCapitalAnalysis(file_path)

    try:
        # Cháº¡y tá»«ng bÆ°á»›c phÃ¢n tÃ­ch
        analyzer.load_and_explore_data()
        analyzer.process_data()
        analyzer.cluster_analysis()
        analyzer.run_econometric_models()
        analyzer.create_visualizations()
        analyzer.machine_learning_analysis()
        analyzer.generate_final_report()

        print("\nğŸ‰ PhÃ¢n tÃ­ch hoÃ n táº¥t thÃ nh cÃ´ng!")

        return analyzer

    except FileNotFoundError:
        print("âŒ Lá»—i: KhÃ´ng tÃ¬m tháº¥y file dá»¯ liá»‡u!")
        print("   Vui lÃ²ng kiá»ƒm tra Ä‘Æ°á»ng dáº«n file trong biáº¿n 'file_path'")
        return None
    except Exception as e:
        print(f"âŒ Lá»—i trong quÃ¡ trÃ¬nh phÃ¢n tÃ­ch: {str(e)}")
        print("   Vui lÃ²ng kiá»ƒm tra dá»¯ liá»‡u vÃ  thá»­ láº¡i")
        return None


# HÃ m tiá»‡n Ã­ch Ä‘á»ƒ cháº¡y phÃ¢n tÃ­ch nhanh
def quick_analysis(file_path, save_results=False):
    """
    Cháº¡y phÃ¢n tÃ­ch nhanh vá»›i cÃ¡c tÃ¹y chá»n cÆ¡ báº£n

    Parameters:
    -----------
    file_path : str
        ÄÆ°á»ng dáº«n Ä‘áº¿n file dá»¯ liá»‡u
    save_results : bool
        CÃ³ lÆ°u káº¿t quáº£ khÃ´ng (default: False)

    Returns:
    --------
    analyzer : ICTIntellectualCapitalAnalysis
        Object chá»©a káº¿t quáº£ phÃ¢n tÃ­ch
    """
    analyzer = ICTIntellectualCapitalAnalysis(file_path)

    try:
        print("ğŸ”„ Äang cháº¡y phÃ¢n tÃ­ch nhanh...")

        # Load vÃ  process data
        analyzer.load_and_explore_data()
        analyzer.process_data()

        # Clustering
        analyzer.cluster_analysis()

        # Econometric models
        analyzer.run_econometric_models()

        # Visualizations
        analyzer.create_visualizations()

        # ML analysis
        analyzer.machine_learning_analysis()

        # Final report
        analyzer.generate_final_report()

        # LÆ°u káº¿t quáº£ náº¿u Ä‘Æ°á»£c yÃªu cáº§u
        if save_results:
            save_analysis_results(analyzer)

        return analyzer

    except Exception as e:
        print(f"âŒ Lá»—i trong quick_analysis: {str(e)}")
        return None


def save_analysis_results(analyzer, output_dir="analysis_results"):
    """
    LÆ°u káº¿t quáº£ phÃ¢n tÃ­ch ra file

    Parameters:
    -----------
    analyzer : ICTIntellectualCapitalAnalysis
        Object chá»©a káº¿t quáº£ phÃ¢n tÃ­ch
    output_dir : str
        ThÆ° má»¥c lÆ°u káº¿t quáº£
    """
    import os

    # Táº¡o thÆ° má»¥c náº¿u chÆ°a cÃ³
    os.makedirs(output_dir, exist_ok=True)

    try:
        # LÆ°u processed data
        if analyzer.df_processed is not None:
            analyzer.df_processed.to_csv(
                f"{output_dir}/processed_data.csv", index=False
            )
            print(f"âœ… ÄÃ£ lÆ°u processed data: {output_dir}/processed_data.csv")

        # LÆ°u cluster results
        if analyzer.clusters_df is not None:
            analyzer.clusters_df.to_csv(
                f"{output_dir}/cluster_results.csv", index=False
            )
            print(f"âœ… ÄÃ£ lÆ°u cluster results: {output_dir}/cluster_results.csv")

        # LÆ°u model results summary
        if analyzer.model_results:
            with open(f"{output_dir}/model_summary.txt", "w", encoding="utf-8") as f:
                f.write("SUMMARY OF ECONOMETRIC MODELS\n")
                f.write("=" * 50 + "\n")

                for model_name, results in analyzer.model_results.items():
                    f.write(f"\n{model_name.upper()}:\n")
                    f.write("-" * 30 + "\n")
                    try:
                        f.write(str(results.summary))
                        f.write("\n" + "=" * 50 + "\n")
                    except Exception as e:
                        f.write(f"Could not save model summary: {e}\n")

            print(f"âœ… ÄÃ£ lÆ°u model summary: {output_dir}/model_summary.txt")

        print(f"ğŸ“ Táº¥t cáº£ káº¿t quáº£ Ä‘Ã£ Ä‘Æ°á»£c lÆ°u trong thÆ° má»¥c: {output_dir}/")

    except Exception as e:
        print(f"âŒ Lá»—i khi lÆ°u káº¿t quáº£: {str(e)}")


# HÃ m Ä‘á»ƒ táº¡o data máº«u (náº¿u cáº§n test)
def create_sample_data(n_countries=20, n_years=10, filename="sample_ict_data.csv"):
    """
    Táº¡o dá»¯ liá»‡u máº«u Ä‘á»ƒ test code

    Parameters:
    -----------
    n_countries : int
        Sá»‘ lÆ°á»£ng quá»‘c gia
    n_years : int
        Sá»‘ nÄƒm
    filename : str
        TÃªn file output
    """
    np.random.seed(42)

    countries = [f"Country_{i + 1}" for i in range(n_countries)]
    years = list(range(2014, 2014 + n_years))

    data = []

    for country in countries:
        base_gdp = np.random.uniform(1e10, 1e12)  # Base GDP
        base_pop = np.random.uniform(1e6, 1e8)  # Base population

        for year in years:
            # TÄƒng trÆ°á»Ÿng theo thá»i gian
            growth_factor = (year - 2014) * 0.02 + np.random.normal(0, 0.05)

            row = {
                "Time": year,
                "Country Name": country,
                "Country Code": country[:3].upper(),
                "gdp": base_gdp * (1 + growth_factor),
                "hte": np.random.uniform(0.5, 0.9),
                "ict_exp": np.random.uniform(2, 8),
                "fdi": np.random.uniform(-5, 15),
                "inet_usr": np.random.uniform(20, 95),
                "sec_srv": np.random.uniform(1, 100),
                "mob_sub": np.random.uniform(50, 150),
                "ter_enr": np.random.uniform(10, 80),
                "edu_exp": np.random.uniform(3, 7),
                "rnd_exp": np.random.uniform(0.5, 4),
                "sci_art": np.random.uniform(100, 5000),
                "hci": np.random.uniform(0.4, 0.8),
                "pop": base_pop * (1 + growth_factor * 0.5),
                "infl": np.random.uniform(-2, 10),
                "urb_area": np.random.uniform(30, 95),
                "trade": np.random.uniform(20, 200),
            }
            data.append(row)

    df = pd.DataFrame(data)
    df.to_csv(filename, index=False)
    print(f"âœ… ÄÃ£ táº¡o dá»¯ liá»‡u máº«u: {filename}")
    print(f"   KÃ­ch thÆ°á»›c: {df.shape}")
    print(f"   Quá»‘c gia: {n_countries}, NÄƒm: {n_years}")

    return df


# Cháº¡y phÃ¢n tÃ­ch chÃ­nh
if __name__ == "__main__":
    # HÆ¯á»šNG DáºªN Sá»¬ Dá»¤NG:
    # 1. Thay Ä‘á»•i Ä‘Æ°á»ng dáº«n file dá»¯ liá»‡u cá»§a báº¡n
    # 2. Cháº¡y script nÃ y

    # TÃ¹y chá»n 1: Sá»­ dá»¥ng dá»¯ liá»‡u tháº­t
    # file_path = "path/to/your/data.xlsx"  # Thay báº±ng Ä‘Æ°á»ng dáº«n tháº­t
    # analyzer = main()

    # TÃ¹y chá»n 2: Táº¡o vÃ  sá»­ dá»¥ng dá»¯ liá»‡u máº«u Ä‘á»ƒ test
    print("ğŸ”§ Táº¡o dá»¯ liá»‡u máº«u Ä‘á»ƒ test...")
    create_sample_data(n_countries=25, n_years=8, filename="sample_ict_data.csv")

    print("\nğŸš€ Cháº¡y phÃ¢n tÃ­ch vá»›i dá»¯ liá»‡u máº«u...")
    analyzer = main()

    # TÃ¹y chá»n 3: Sá»­ dá»¥ng quick_analysis
    # analyzer = quick_analysis("sample_ict_data.csv", save_results=True)

print("""
ğŸ“‹ HÆ¯á»šNG DáºªN Sá»¬ Dá»¤NG:

1. CÃ i Ä‘áº·t cÃ¡c thÆ° viá»‡n cáº§n thiáº¿t:
   pip install pandas numpy matplotlib seaborn scikit-learn statsmodels linearmodels

2. Chuáº©n bá»‹ dá»¯ liá»‡u:
   - File Excel hoáº·c CSV vá»›i cÃ¡c cá»™t theo header báº¡n cung cáº¥p
   - Äáº£m báº£o cÃ³ Ä‘á»§ dá»¯ liá»‡u cho phÃ¢n tÃ­ch

3. Cháº¡y phÃ¢n tÃ­ch:
   - Thay Ä‘á»•i Ä‘Æ°á»ng dáº«n file trong biáº¿n file_path
   - Cháº¡y script: python your_script_name.py

4. CÃ¡c hÃ m tiá»‡n Ã­ch:
   - main(): Cháº¡y phÃ¢n tÃ­ch Ä‘áº§y Ä‘á»§
   - quick_analysis(): Cháº¡y phÃ¢n tÃ­ch nhanh
   - create_sample_data(): Táº¡o dá»¯ liá»‡u máº«u
   - save_analysis_results(): LÆ°u káº¿t quáº£

5. Káº¿t quáº£ sáº½ bao gá»“m:
   - Thá»‘ng kÃª mÃ´ táº£ vÃ  khÃ¡m phÃ¡ dá»¯ liá»‡u
   - PhÃ¢n nhÃ³m quá»‘c gia (clustering)
   - Káº¿t quáº£ cÃ¡c mÃ´ hÃ¬nh kinh táº¿ lÆ°á»£ng
   - Biá»ƒu Ä‘á»“ trá»±c quan hÃ³a
   - PhÃ¢n tÃ­ch Machine Learning
   - BÃ¡o cÃ¡o tá»•ng há»£p vÃ  Ä‘á» xuáº¥t

ğŸ’¡ LÆ°u Ã½: Script nÃ y Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ xá»­ lÃ½ dá»¯ liá»‡u panel vá»›i cáº¥u trÃºc
quá»‘c gia-thá»i gian vÃ  cÃ¡c biáº¿n ICT, Intellectual Capital theo nghiÃªn cá»©u cá»§a báº¡n.
""")
